{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac4d8e2-b2a7-48b2-97f1-f2e13d11c5d6",
   "metadata": {},
   "source": [
    "#### `RAG = Retrieval-Augmented Generation`\n",
    "\n",
    "It’s an architecture that combines information retrieval (like search) with generative AI (like LLMs such as GPT, LLaMA, etc.) to answer user queries with context-aware, accurate, and source-grounded responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ab31d-7209-4374-a5fd-d5b6a9cfbfa0",
   "metadata": {},
   "source": [
    "## RAG Architecture"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a5b969c-ea14-43a7-9f15-40a85970b394",
   "metadata": {},
   "source": [
    "            ┌────────────┐\n",
    "  Query ──▶ │ Embed Query│────────────┐\n",
    "            └────────────┘            │\n",
    "                                      ▼\n",
    "                             ┌────────────────┐\n",
    "                             │Vector Search DB│  ← ChromaDB with .md files\n",
    "                             └────────────────┘\n",
    "                                      │\n",
    "                       Top K Matches │\n",
    "                                      ▼\n",
    "                     ┌────────────────────────┐\n",
    "                     │Build Prompt with Context│\n",
    "                     └────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "                      ┌────────────────────────────┐\n",
    "                      │Send to LLM (Groq / LLaMA3) │\n",
    "                      └────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "                    Answer with source-based response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280dfc2-c752-4c55-966f-69911977105e",
   "metadata": {},
   "source": [
    "## Chroma \n",
    "\n",
    "`Chroma` is an AI-native open-source `vector database` It comes with everything you need to get started built in, and runs on your machine. A hosted version is now available for early access!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ffdaf8-a2c5-4165-8175-d982d01462d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3703ca5b-65fa-4659-a275-ed0dbb8871c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"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\"  \n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad655908-a0db-4207-8248-1cb96eea44be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All documents embedded and stored.\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./vector_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"company_docs\")\n",
    "\n",
    "base_path = \"data\"\n",
    "doc_id = 0\n",
    "\n",
    "for folder in os.listdir(base_path):\n",
    "    role_path = os.path.join(base_path, folder)\n",
    "    if os.path.isdir(role_path):\n",
    "        for file in glob.glob(f\"{role_path}/*.md\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            embedding = embedding_model.encode(text).tolist()\n",
    "            collection.add(\n",
    "                documents=[text],\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\"role\": folder, \"source\": os.path.basename(file)}],\n",
    "                ids=[f\"doc_{doc_id}\"]\n",
    "            )\n",
    "            doc_id += 1\n",
    "\n",
    "print(\"✅ All documents embedded and stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d410e9b-cda0-41cc-8778-70a2458081e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, user_role, top_k=3):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        where={\"role\": user_role}\n",
    "    )\n",
    "    docs = results['documents'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "    sources = [md['source'] for md in metadatas]\n",
    "    return docs, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1092d2dc-fa6c-473f-b5e6-7027a4016fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_groq_rag(query, user_role):\n",
    "    docs, sources = retrieve_docs(query, user_role)\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant answering questions for the {user_role} department.\n",
    "\n",
    "Use only the context below to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant for the {user_role} team.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fdc7729-32db-43ac-99bd-a0bd93c8e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop(user_role):\n",
    "    print(f\"💬 Chatbot active for role: {user_role.capitalize()}\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"👋 Chatbot: Goodbye!\")\n",
    "            break\n",
    "        response, sources = ask_groq_rag(user_input, user_role)\n",
    "        print(f\"\\n🧠 Chatbot:\\n{response}\")\n",
    "        print(f\"\\n📎 Sources: {', '.join(sources)}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf41da0-bcdc-4d36-a87e-168ad6aee952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Chatbot active for role: Finance\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop(\"finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d746029-431a-4231-b6de-56ae224c3aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
